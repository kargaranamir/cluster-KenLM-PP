{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kargaranamir/cluster-KenLM-PP/blob/main/cluster_kenlm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "vwTXIwWC1llW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/kpu/kenlm.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lO8p-7nAGcDh",
        "outputId": "387ee015-8043-44cf-9b14-f096d0b7bf71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'kenlm'...\n",
            "remote: Enumerating objects: 14142, done.\u001b[K\n",
            "remote: Counting objects: 100% (455/455), done.\u001b[K\n",
            "remote: Compressing objects: 100% (317/317), done.\u001b[K\n",
            "remote: Total 14142 (delta 149), reused 394 (delta 124), pack-reused 13687\u001b[K\n",
            "Receiving objects: 100% (14142/14142), 5.91 MiB | 5.01 MiB/s, done.\n",
            "Resolving deltas: 100% (8029/8029), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mv kenlm kenlmgit"
      ],
      "metadata": {
        "id": "U4oU9cwtw3Wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/kenlmgit\n",
        "!mkdir -p build"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqzqJHTDwznM",
        "outputId": "24e295aa-d823-49da-a1cd-53ae8437a4ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/kenlmgit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTvmT_B1F_Jw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ea23c86-6f47-4ee0-894a-9a183fd89719"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/kenlmgit/build\n",
            "-- The C compiler identification is GNU 9.4.0\n",
            "-- The CXX compiler identification is GNU 9.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Could NOT find Eigen3 (missing: Eigen3_DIR)\n",
            "-- Found Boost: /usr/lib/x86_64-linux-gnu/cmake/Boost-1.71.0/BoostConfig.cmake (found suitable version \"1.71.0\", minimum required is \"1.41.0\") found components: program_options system thread unit_test_framework \n",
            "-- Check if compiler accepts -pthread\n",
            "-- Check if compiler accepts -pthread - yes\n",
            "-- Found Threads: TRUE  \n",
            "-- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version \"1.2.11\") \n",
            "-- Found BZip2: /usr/lib/x86_64-linux-gnu/libbz2.so (found version \"1.0.8\") \n",
            "-- Looking for BZ2_bzCompressInit\n",
            "-- Looking for BZ2_bzCompressInit - found\n",
            "-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
            "-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
            "-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
            "-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
            "-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
            "-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
            "-- Found LibLZMA: /usr/lib/x86_64-linux-gnu/liblzma.so (found version \"5.2.4\") \n",
            "-- Looking for clock_gettime in rt\n",
            "-- Looking for clock_gettime in rt - found\n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/kenlmgit/build\n"
          ]
        }
      ],
      "source": [
        "%cd /content/kenlmgit/build\n",
        "!cmake .."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!make -j 8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKonklbOGtS6",
        "outputId": "c08563e2-10c9-4f4c-b635-0788cd33de9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  1%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum-dtoa.cc.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum.cc.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fast-dtoa.cc.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/cached-powers.cc.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fixed-dtoa.cc.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/strtod.cc.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/string-to-double.cc.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/double-to-string.cc.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/chain.cc.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/count_records.cc.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/io.cc.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/line_input.cc.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/multi_progress.cc.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/rewindable_stream.cc.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/bit_packing.cc.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/ersatz_progress.cc.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/exception.cc.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file.cc.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file_piece.cc.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/float_to_string.cc.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/integer_to_string.cc.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/mmap.cc.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/murmur_hash.cc.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/parallel_read.cc.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/pool.cc.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/read_compressed.cc.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/scoped.cc.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/spaces.cc.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/string_piece.cc.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/usage.cc.o\u001b[0m\n",
            "[ 38%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm_util.a\u001b[0m\n",
            "[ 38%] Built target kenlm_util\n",
            "[ 40%] \u001b[32mBuilding CXX object util/CMakeFiles/probing_hash_table_benchmark.dir/probing_hash_table_benchmark_main.cc.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/bhiksha.cc.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/arpa_io.cc.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/binary_format.cc.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/lm_exception.cc.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/model.cc.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/config.cc.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/phrase.cc.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/quantize.cc.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/read_arpa.cc.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_hashed.cc.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_trie.cc.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/vocab.cc.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/sizes.cc.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie.cc.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie_sort.cc.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/value_build.cc.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/virtual_interface.cc.o\u001b[0m\n",
            "[ 62%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_filter.a\u001b[0m\n",
            "[ 62%] Built target kenlm_filter\n",
            "[ 63%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/vocab.cc.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/model_buffer.cc.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/print.cc.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/renumber.cc.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/size_option.cc.o\u001b[0m\n",
            "[ 70%] \u001b[32m\u001b[1mLinking CXX executable ../bin/probing_hash_table_benchmark\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm.a\u001b[0m\n",
            "[ 71%] Built target probing_hash_table_benchmark\n",
            "[ 71%] Built target kenlm\n",
            "[ 72%] \u001b[32mBuilding CXX object lm/CMakeFiles/query.dir/query_main.cc.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object lm/CMakeFiles/build_binary.dir/build_binary_main.cc.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm_benchmark.dir/kenlm_benchmark_main.cc.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object lm/CMakeFiles/fragment.dir/fragment_main.cc.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/adjust_counts.cc.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/corpus_count.cc.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/phrase_table_vocab.dir/phrase_table_vocab_main.cc.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/filter.dir/filter_main.cc.o\u001b[0m\n",
            "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../bin/fragment\u001b[0m\n",
            "[ 82%] Built target fragment\n",
            "[ 83%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/initial_probabilities.cc.o\u001b[0m\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../bin/build_binary\u001b[0m\n",
            "[ 85%] Built target build_binary\n",
            "[ 86%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/interpolate.cc.o\u001b[0m\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../bin/query\u001b[0m\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/phrase_table_vocab\u001b[0m\n",
            "[ 88%] Built target query\n",
            "[ 90%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/output.cc.o\u001b[0m\n",
            "[ 90%] Built target phrase_table_vocab\n",
            "[ 91%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/pipeline.cc.o\u001b[0m\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_builder.a\u001b[0m\n",
            "[ 92%] Built target kenlm_builder\n",
            "[ 93%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/lmplz.dir/lmplz_main.cc.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/count_ngrams.dir/count_ngrams_main.cc.o\u001b[0m\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/filter\u001b[0m\n",
            "[ 96%] Built target filter\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../bin/kenlm_benchmark\u001b[0m\n",
            "[ 97%] Built target kenlm_benchmark\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/lmplz\u001b[0m\n",
            "[ 98%] Built target lmplz\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/count_ngrams\u001b[0m\n",
            "[100%] Built target count_ngrams\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/kenlmgit\n",
        "! pip install .\n",
        "%cd /content"
      ],
      "metadata": {
        "id": "ALmh8s-vyYAI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1775a7ac-95c3-40e5-f5e7-e455262284fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/kenlmgit\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/kenlmgit\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: kenlm\n",
            "  Building wheel for kenlm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kenlm: filename=kenlm-0.0.0-cp39-cp39-linux_x86_64.whl size=3262389 sha256=caa7eff9e169c788964bc02182067fa9aef52dccf678c1d659a9af7d47fb782f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-874w_cu8/wheels/ba/1a/d9/1e191f320910c64fcc8e713de08c5132cbf9f549ec2a91f5b2\n",
            "Successfully built kenlm\n",
            "Installing collected packages: kenlm\n",
            "Successfully installed kenlm-0.0.0\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create file\n",
        "!echo -e \"hello world everyone\\ngood afternoon ladies and gentleman\\nmy friend in college\\nhave a nice day\\nspy and family at the same time\\nwe do not do that here \" >> example.txt"
      ],
      "metadata": {
        "id": "aMFCovDpHjid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# you don't need --discount_fallback if you use your real data (which may huge)\n",
        "!kenlmgit/build/bin/lmplz -o 2 -S 80% -T /tmp <example.txt >example.arpa --discount_fallback"
      ],
      "metadata": {
        "id": "WRxP6dVkJALg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "109b14e9-dbba-4fc0-efb6-ca86c07d25f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1/5 Counting and sorting n-grams ===\n",
            "Reading /content/example.txt\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Unigram tokens 29 types 30\n",
            "=== 2/5 Calculating and sorting adjusted counts ===\n",
            "Chain sizes: 1:360 2:10893066240\n",
            "Substituting fallback discounts for order 0: D1=0.5 D2=1 D3+=1.5\n",
            "Substituting fallback discounts for order 1: D1=0.5 D2=1 D3+=1.5\n",
            "Statistics:\n",
            "1 30 D1=0.5 D2=1 D3+=1.5\n",
            "2 35 D1=0.5 D2=1 D3+=1.5\n",
            "Memory estimate for binary LM:\n",
            "type       B\n",
            "probing 1420 assuming -p 1.5\n",
            "probing 1544 assuming -r models -p 1.5\n",
            "trie     930 without quantization\n",
            "trie    1859 assuming -q 8 -b 8 quantization \n",
            "trie     930 assuming -a 22 array pointer compression\n",
            "trie    1859 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
            "=== 3/5 Calculating and sorting initial probabilities ===\n",
            "Chain sizes: 1:360 2:560\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
            "Chain sizes: 1:360 2:560\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 5/5 Writing ARPA model ===\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Name:lmplz\tVmPeak:10785392 kB\tVmRSS:6260 kB\tRSSMax:2900236 kB\tuser:0.242133\tsys:1.61168\tCPU:1.85384\treal:1.94645\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! rm example.arpa\n",
        "! rm example.txt"
      ],
      "metadata": {
        "id": "8k4N2Dxm2V6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main"
      ],
      "metadata": {
        "id": "WZVctKhu1j3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "import kenlm\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "import collections, functools, operator\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import re"
      ],
      "metadata": {
        "id": "Pz46B6na2mcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Langcluster:\n",
        "    def __init__(self, ngram_param, num_models, lmplz_path, dataset_path, models_path, split_data_path):\n",
        "        self.ngram_param = int(ngram_param)\n",
        "        self.num_models = int(num_models)\n",
        "        self.lmplz_path = lmplz_path.rstrip('/')\n",
        "        self.dataset_path = dataset_path.rstrip('/')\n",
        "        self.models_path = models_path.rstrip('/')\n",
        "        self.split_data_path = split_data_path.rstrip('/')\n",
        "        self.target_data = []\n",
        "        self.target_data_dict = {}\n",
        "        self.size_data = 0\n",
        "        self.models_dict = {} # {0: {indices: [], class: }, 1: [], 2: [], ....}\n",
        "        self.BIG_PREP = 1000\n",
        "        self.MAX_ITERATION = 1000_000\n",
        "        self.NUM_SENTENCE_FIRST = 10\n",
        "        self.NUM_SENTENCE_MORE = 10\n",
        "\n",
        "\n",
        "        # create paths\n",
        "        Path(models_path).mkdir(parents=True, exist_ok=True)\n",
        "        Path(split_data_path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "    def read_main_file(self, header=True):\n",
        "        # read target data\n",
        "        self.target_data = []\n",
        "        with open(self.dataset_path, 'r') as input_data:\n",
        "            for index, line in enumerate(input_data):\n",
        "                if index!=0 or (index==0 and header==False):\n",
        "                    self.target_data.append(line.strip())\n",
        "                else:\n",
        "                    pass\n",
        "\n",
        "        # update size of target data\n",
        "        self.size_data = len(self.target_data)\n",
        "        # convert data to dict\n",
        "        self.convert_data_list_to_dict()\n",
        "\n",
        "        return True\n",
        "\n",
        "    def select_random_sent_seeds(self):\n",
        "        selcted_seeds = random.sample(range(0, self.size_data), self.num_models)\n",
        "        res_dict = {}\n",
        "        for i in range(0, self.num_models):\n",
        "            res_dict[i] = {}\n",
        "            res_dict[i]['indices'] = []\n",
        "            res_dict[i]['indices'].append(selcted_seeds[i])\n",
        "            res_dict[i]['class'] = self.target_data[selcted_seeds[i]].strip().split('|')[0].strip()\n",
        "            res_dict[i]['class_indices'] = [res_dict[i]['class']]\n",
        "            res_dict[i]['index_plus_class'] = [f\"{selcted_seeds[i]}: {res_dict[i]['class']}\"]\n",
        "\n",
        "        return res_dict\n",
        "\n",
        "    def save_splitted_data(self):\n",
        "\n",
        "        for key in self.models_dict.keys():\n",
        "            data_indices_model_i = self.models_dict[key]['indices']\n",
        "            data_model_i = []\n",
        "\n",
        "            for sent_index in data_indices_model_i:\n",
        "                sent = '|'.join(self.target_data[sent_index].strip().split('|')[1:]).strip()\n",
        "                data_model_i.append(sent)\n",
        "\n",
        "            with open(f\"{self.split_data_path}/data_model_{key}\", 'w') as f:\n",
        "                f.write(\"\\n\".join(data_model_i))\n",
        "                f.write(\"\\n\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def train_splitted_data_models(self):\n",
        "        for key in tqdm(self.models_dict.keys()):\n",
        "\n",
        "            command = ' '.join([f\"{self.lmplz_path}\", f\"-o {str(self.ngram_param)}\", f\"<{self.split_data_path}/data_model_{key}\", f\">{self.models_path}/model_{key}.arpa\", \"--discount_fallback\"])\n",
        "            # print(f\"\\n **Model_NO_{key} train started**\")\n",
        "            os.system(command)\n",
        "            # print(f\"\\n **Model_NO_{key} train finished**\")\n",
        "\n",
        "        return True\n",
        "\n",
        "\n",
        "    def convert_data_list_to_dict(self):\n",
        "        self.target_data_dict = {}\n",
        "\n",
        "        for index, sent in enumerate(self.target_data):\n",
        "            self.target_data_dict[index] = {}\n",
        "            self.target_data_dict[index]['_sent'] = '|'.join(self.target_data[index].strip().split('|')[1:]).strip()\n",
        "            self.target_data_dict[index]['_class'] = self.target_data[index].strip().split('|')[0].strip()\n",
        "\n",
        "        return True\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def pp(log_score, length):\n",
        "        return 10.0 ** (-log_score / length)\n",
        "\n",
        "\n",
        "    def prepchar(self, model, model_lang, docs_dict_char):\n",
        "\n",
        "        for doc_key in docs_dict_char.keys():\n",
        "            try:\n",
        "                line = docs_dict_char[doc_key]['_sent']\n",
        "                log_score = model.score(line, bos=True, eos=True)\n",
        "                length = len(line.split()) + 1\n",
        "\n",
        "                prep_score = round(self.pp(log_score, length), 2)\n",
        "                docs_dict_char[doc_key][model_lang] = prep_score\n",
        "            except:\n",
        "                docs_dict_char[doc_key][model_lang] = self.BIG_PREP\n",
        "\n",
        "        return docs_dict_char\n",
        "\n",
        "\n",
        "    def evaluate_models(self):\n",
        "        input_model_paths = glob(f\"{self.models_path}/*\")\n",
        "        for m_path in tqdm(input_model_paths):\n",
        "            # open model\n",
        "            model = kenlm.Model(m_path)\n",
        "            self.target_data_dict = self.prepchar(model, m_path, self.target_data_dict)\n",
        "\n",
        "        return True\n",
        "\n",
        "\n",
        "    def compute_confidence(self):\n",
        "        # compute confidence for each model and sentence\n",
        "        input_model_paths = glob(f\"{self.models_path}/*\")\n",
        "        for key in self.target_data_dict.keys():\n",
        "            perp_models = [float(self.target_data_dict[int(key)][str(m_path)]) for m_path in input_model_paths]\n",
        "            perp_models_sorted = sorted(perp_models)\n",
        "            perfromance_key = perp_models_sorted[0] / (perp_models_sorted[1] - perp_models_sorted[0] + + 0.0001)\n",
        "            min_model_index = int(perp_models.index(perp_models_sorted[0]))\n",
        "\n",
        "            self.target_data_dict[key]['_min_model'] = str(input_model_paths[min_model_index])\n",
        "            self.target_data_dict[key]['_min_model_performance'] = float(perfromance_key)\n",
        "\n",
        "        return True\n",
        "\n",
        "    def select_next_candidates(self, num_top_sent=10):\n",
        "\n",
        "        input_dict = self.models_dict.copy()\n",
        "        total_selected_indices = []\n",
        "\n",
        "        for key in input_dict.keys():\n",
        "            total_selected_indices = total_selected_indices + input_dict[key]['indices']\n",
        "\n",
        "        sorted_choices = list(sorted(self.target_data_dict, key=lambda x:self.target_data_dict[x]['_min_model_performance'], reverse=False))\n",
        "\n",
        "        # remove selected ones:\n",
        "        for num in total_selected_indices:\n",
        "            sorted_choices.remove(num)\n",
        "\n",
        "\n",
        "        candidates_top = sorted_choices[:num_top_sent]\n",
        "        for num in candidates_top:\n",
        "            model_no = int(self.target_data_dict[num]['_min_model'].split('/')[-1].replace('.arpa', '').split('_')[-1])\n",
        "            sent_script_class = self.target_data_dict[num]['_class']\n",
        "            input_dict[model_no]['indices'].append(num)\n",
        "            input_dict[model_no]['class_indices'].append(sent_script_class)\n",
        "            input_dict[model_no]['index_plus_class'].append(f\"{num}: {sent_script_class}\")\n",
        "\n",
        "\n",
        "        return input_dict\n",
        "\n",
        "\n",
        "    def first_iteration(self):\n",
        "        self.read_main_file(header=True)\n",
        "        self.models_dict = self.select_random_sent_seeds()\n",
        "        self.save_splitted_data()\n",
        "        self.train_splitted_data_models()\n",
        "        self.evaluate_models()\n",
        "        self.compute_confidence()\n",
        "        self.models_dict = self.select_next_candidates(num_top_sent=self.NUM_SENTENCE_FIRST)\n",
        "\n",
        "\n",
        "    def more_iteration(self, num_iteration=1000_000):\n",
        "        for _ in tqdm(range(0, num_iteration)):\n",
        "\n",
        "            prev_models_dict = copy.deepcopy(self.models_dict)\n",
        "\n",
        "            # save selected data\n",
        "            self.save_splitted_data()\n",
        "            # train models\n",
        "            self.train_splitted_data_models()\n",
        "            # evaluate models\n",
        "            self.evaluate_models()\n",
        "            # compute confidence\n",
        "            self.compute_confidence()\n",
        "            # assign next sentences\n",
        "            self.models_dict = self.select_next_candidates(num_top_sent=self.NUM_SENTENCE_MORE)\n",
        "\n",
        "            if self.models_dict == prev_models_dict:\n",
        "                return True\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        self.first_iteration()\n",
        "        self.more_iteration(num_iteration=self.MAX_ITERATION)\n",
        "\n",
        "    def predict(self):\n",
        "        input_model_paths = glob(f\"{self.models_path}/*\")\n",
        "        model_paths_no = [int(key.split('/')[-1].replace('.arpa', '').split('_')[-1]) for key in input_model_paths]\n",
        "\n",
        "        eval_model = {}\n",
        "        for model_no in model_paths_no:\n",
        "            model_i_indices = self.models_dict[model_no]['indices']\n",
        "\n",
        "            # get dicts of sentences as a list\n",
        "            sent_dicts_i = [self.target_data_dict[sent_i] for sent_i in model_i_indices]\n",
        "            # only the models keys\n",
        "            sent_dicts_i_restricted = [{key: item[key] for key in input_model_paths} for item in sent_dicts_i]\n",
        "            # sum over list of dicts\n",
        "            eval_result_i = dict(functools.reduce(operator.add, map(collections.Counter, sent_dicts_i_restricted)))\n",
        "            # mean\n",
        "            eval_result_i_mean = {k: v / len(model_i_indices) for k, v in eval_result_i.items()}\n",
        "\n",
        "            # rename keys\n",
        "            for o_key, n_key in zip(input_model_paths, model_paths_no):\n",
        "                eval_result_i_mean[n_key] = eval_result_i_mean.pop(o_key)\n",
        "\n",
        "            # save\n",
        "            eval_model[model_no] = eval_result_i_mean # {0: {0:8, 1:9, ....}}\n",
        "\n",
        "        return eval_model\n"
      ],
      "metadata": {
        "id": "qKDp-CS_nbBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert setntence to characters\n",
        "def tokenizer_char(sentence: str) -> str:\n",
        "    if sentence == \"\":\n",
        "        return \"\"\n",
        "    elif sentence.strip() == \"\":\n",
        "        return \"<sp>\"\n",
        "    else:\n",
        "        s = ' '.join(list(sentence))\n",
        "        # replace space with <space> token\n",
        "        s = re.sub(r\"\\s\\s+\", ' <sp> ', s)\n",
        "        return s.strip()"
      ],
      "metadata": {
        "id": "0kC-fHiG4AQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def char_to_word(line):\n",
        "    line_list = line.split()\n",
        "    line_list = [x if x!='<sp>' else ' ' for x in  line_list]\n",
        "    return ''.join(line_list)"
      ],
      "metadata": {
        "id": "VOvsifu14BB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir data\n",
        "! rm -r split_data\n",
        "! rm -r models\n",
        "! mkdir split_data\n",
        "! mkdir models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCXAxcdi6eua",
        "outputId": "153bf63c-cc53-4536-bd37-643a03dc2a06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'split_data': No such file or directory\n",
            "rm: cannot remove 'models': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data_df = pd.read_csv('/content/data/tgk_Arab.csv')\n",
        "# data_df['_sent_char'] = data_df['_sent'].apply(lambda x: tokenizer_char(str(x)))\n",
        "# new_data_df = data_df[['0_source_lang', '_sent_char']][:100]\n",
        "# new_data_df.to_csv('/content/data/tgk_Arab', sep='|', index=False)"
      ],
      "metadata": {
        "id": "kApqj76nkSX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tgk_obj = Langcluster(ngram_param=3,\n",
        "#                       num_models=10,\n",
        "#                       lmplz_path = \"/content/kenlmgit/build/bin/lmplz\",\n",
        "#                       dataset_path = \"/content/data/tgk_Arab\",\n",
        "#                       models_path = \"/content/models/tgk_Arab\",\n",
        "#                       split_data_path = \"/content/split_data/tgk_Arab\")"
      ],
      "metadata": {
        "id": "54JogcpA0u00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tgk_obj.train()"
      ],
      "metadata": {
        "id": "A8V7d0y67wv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # predict 10X10 and save\n",
        "# res_predict = tgk_obj.predict()\n",
        "# df_res_predict = pd.DataFrame(res_predict)\n",
        "# df_res_predict = df_res_predict.sort_index(axis=1)\n",
        "# df_res_predict = df_res_predict.sort_index(axis=0)\n",
        "# df_res_predict.to_csv(f'/content/split_data/tgk_Arab/log.csv')\n",
        "# ! zip -r split_data_tgk_Arab.zip /content/split_data/tgk_Arab\n",
        "# files.download('split_data_tgk_Arab.zip')"
      ],
      "metadata": {
        "id": "d1O0xWOAx3jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Practice"
      ],
      "metadata": {
        "id": "CBLjk6i2tfTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def practice(language_name):\n",
        "\n",
        "    # prepare data\n",
        "    data_df = pd.read_csv(f'/content/data/{language_name}.csv')\n",
        "    data_df['_sent_char'] = data_df['_sent'].apply(lambda x: tokenizer_char(str(x)))\n",
        "    new_data_df = data_df[['0_source_lang', '_sent_char']][:1000]\n",
        "    new_data_df.to_csv(f'/content/data/{language_name}', sep='|', index=False)\n",
        "\n",
        "\n",
        "    # create object\n",
        "    obj = Langcluster(ngram_param=3,\n",
        "                          num_models=10,\n",
        "                          lmplz_path = \"/content/kenlmgit/build/bin/lmplz\",\n",
        "                          dataset_path = f\"/content/data/{language_name}\",\n",
        "                          models_path = f\"/content/models/{language_name}\",\n",
        "                          split_data_path = f\"/content/split_data/{language_name}\")\n",
        "\n",
        "    # train\n",
        "    obj.train()\n",
        "\n",
        "    # save data of other models\n",
        "    for model_no in obj.models_dict.keys():\n",
        "        model_indices_i = obj.models_dict[model_no]['indices']\n",
        "        data_df.loc[data_df.index.isin(model_indices_i)].to_csv(f'/content/split_data/{language_name}/data_model_{model_no}.csv')\n",
        "\n",
        "    # predict 10X10 and save\n",
        "    res_predict = obj.predict()\n",
        "    df_res_predict = pd.DataFrame(res_predict)\n",
        "    df_res_predict = df_res_predict.sort_index(axis=1)\n",
        "    df_res_predict = df_res_predict.sort_index(axis=0)\n",
        "    df_res_predict.to_csv(f'/content/split_data/{language_name}/log.csv')\n",
        "\n"
      ],
      "metadata": {
        "id": "63itr1dxraOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "msCclWyyfzT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "practice('ach_Latn')\n",
        "! zip -r split_data_ach_Latn.zip /content/split_data/ach_Latn\n",
        "files.download('split_data_ach_Latn.zip')"
      ],
      "metadata": {
        "id": "XzsTA3_X5PQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "practice('luo_Latn')\n",
        "! zip -r split_data_luo_Latn.zip /content/split_data/luo_Latn\n",
        "files.download('split_data_luo_Latn.zip')"
      ],
      "metadata": {
        "id": "P6c7p2BH5W6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "practice('teo_Latn')\n",
        "! zip -r split_data_teo_Latn.zip /content/split_data/teo_Latn\n",
        "files.download('split_data_teo_Latn.zip')"
      ],
      "metadata": {
        "id": "RS6AF64v5c6f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}